import os
import io
import random
import asyncio
from fastapi import FastAPI, HTTPException, UploadFile, File
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from transformers import (
    pipeline,
    ViTImageProcessor,
    ViTForImageClassification,
    AutoTokenizer,
    AutoModelForSequenceClassification,
)
import torch
from PIL import Image
from dotenv import load_dotenv

# Load environment variables
load_dotenv()
HF_TOKEN = os.getenv("HF_TOKEN")  # Optional, for private models

# FastAPI app
app = FastAPI()

# Enable CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Device
device = 0 if torch.cuda.is_available() else -1

# Lazy-loaded models
models = {
    "news_classifier": None,  # Changed from text_classifier to news_classifier
    "url_tokenizer": None,
    "url_model": None,
    "image_processor": None,
    "image_model": None,
}

# Request schemas
class NewsAnalysisRequest(BaseModel):  # Renamed from TextAnalysisRequest
    text: str  # This will now be news article text

class UrlAnalysisRequest(BaseModel):
    url: str

# Helper: format results for news analysis
def format_news_result(content_type, raw_result, input_text):
    """Format results specifically for news credibility analysis"""
    primary_result = raw_result[0] if isinstance(raw_result, list) else raw_result
    label = primary_result.get("label", "").lower()
    score = primary_result.get("score", 0.5)

    # DeBERTa model labels: typically "FAKE" or "REAL" / "TRUE"
    is_fake_news = "fake" in label
    is_real_news = any(x in label for x in ["real", "true", "reliable"])
    
    # Adjust credibility score based on prediction
    if is_fake_news:
        credibility_score = 1 - score  # Lower credibility for fake news
    else:
        credibility_score = score  # Higher credibility for real news

    return {
        "type": content_type,
        "credibilityScore": credibility_score,
        "analysis": f"The {content_type} has been classified as {primary_result.get('label', 'unknown')} with confidence {score:.2f}. Credibility score: {credibility_score:.2f}",
        "flags": {
            "potentialMisinformation": is_fake_news,
            "needsFactChecking": is_fake_news or credibility_score < 0.7,
            "biasDetected": credibility_score < 0.6,
            "manipulatedContent": is_fake_news,
            "isReliableNews": is_real_news and credibility_score > 0.8,
        },
        "sources": [
            f"DeBERTa News Classifier: {primary_result.get('label', 'N/A')}",
            "Microsoft DeBERTa v3 Base Fine-tuned Model",
            "External News Verification Service"
        ],
        "details": {
            "classification": primary_result.get("label", "Unknown"),
            "confidence": score,
            "credibilityLevel": "High" if credibility_score > 0.7 else "Medium" if credibility_score > 0.4 else "Low",
            "keyTerms": input_text.split()[:10] if input_text else [],  # More key terms for news
            "recommendation": "Verify with multiple sources" if is_fake_news else "Appears credible but always cross-reference"
        }
    }

# Helper: format results for other content types
def format_result(content_type, raw_result, input_text):
    """Original format function for non-news content"""
    primary_result = raw_result[0] if isinstance(raw_result, list) else raw_result
    label = primary_result.get("label", "").lower()
    score = primary_result.get("score", 0.5)

    is_misinfo = any(label.startswith(x) for x in ["misinformation", "fake", "defacement", "malware"])
    is_phishing = any(label.startswith(x) for x in ["phishing", "malicious"])

    if is_misinfo or is_phishing:
        score = 1 - score

    return {
        "type": content_type,
        "credibilityScore": score,
        "analysis": f"The {content_type} has been flagged as potential {primary_result.get('label', 'misinformation')} with confidence {score:.2f}.",
        "flags": {
            "potentialMisinformation": is_misinfo,
            "needsFactChecking": is_misinfo or is_phishing,
            "biasDetected": "bias" in label,
            "manipulatedContent": is_misinfo or is_phishing,
        },
        "sources": [
            f"AI Model: {primary_result.get('label', 'N/A')}",
            "External Verification Service"
        ],
        "details": {
            "sentiment": "Negative" if is_misinfo else "Positive/Neutral",
            "confidence": primary_result.get("score", 0.5),
            "keyTerms": input_text.split()[:5] if input_text else []
        }
    }

# Fallback mock for news
def mock_news_analysis(content_type, input_text):
    """Mock analysis specifically for news content"""
    mock_score = random.uniform(0.3, 0.9)
    is_fake = mock_score < 0.5
    classification = "FAKE" if is_fake else "REAL"
    
    return {
        "type": content_type,
        "credibilityScore": mock_score,
        "analysis": f"Analysis failed: News classification model could not be loaded. This is a mock result showing {classification}.",
        "flags": {
            "potentialMisinformation": is_fake,
            "needsFactChecking": is_fake,
            "biasDetected": random.choice([True, False]),
            "manipulatedContent": is_fake,
            "isReliableNews": not is_fake and mock_score > 0.8,
        },
        "sources": ["Offline Mode (Mock News Classifier)"],
        "details": {
            "classification": classification,
            "confidence": mock_score,
            "credibilityLevel": "Mock",
            "keyTerms": input_text.split()[:10] if input_text else [],
            "recommendation": "Model unavailable - verify manually"
        },
    }

# Original mock function
def mock_analysis(content_type, input_text):
    mock_score = random.uniform(0.3, 0.9)
    is_misinfo = mock_score < 0.6
    return {
        "type": content_type,
        "credibilityScore": mock_score,
        "analysis": "Analysis failed: AI models could not be loaded. This is a mock result.",
        "flags": {
            "potentialMisinformation": is_misinfo,
            "needsFactChecking": is_misinfo,
            "biasDetected": random.choice([True, False]),
            "manipulatedContent": random.choice([True, False]),
        },
        "sources": ["Offline Mode (Mock)"],
        "details": {
            "sentiment": "N/A",
            "confidence": mock_score,
            "keyTerms": input_text.split()[:5] if input_text else []
        },
    }

# Lazy load models - Updated for DeBERTa news classifier
async def load_news_model():
    """Load the DeBERTa-based fake news detection model"""
    if models["news_classifier"] is None:
        try:
            models["news_classifier"] = pipeline(
                "text-classification",
                model="Denyol/FakeNews-deberta-base",  # Updated model
                device=device,
                token=HF_TOKEN  # In case authentication is needed
            )
            print("DeBERTa news classification model loaded successfully")
        except Exception as e:
            print(f"Failed to load DeBERTa news model: {e}")
            # Fallback to original BERT model if DeBERTa fails
            try:
                models["news_classifier"] = pipeline(
                    "text-classification",
                    model="dhruvpal/fake-news-bert",
                    device=device
                )
                print("Fallback: Original BERT news model loaded")
            except Exception as fallback_error:
                print(f"Fallback also failed: {fallback_error}")

async def load_url_model():
    if models["url_tokenizer"] is None or models["url_model"] is None:
        try:
            # Using a public text classifier to avoid token issues
            models["url_tokenizer"] = AutoTokenizer.from_pretrained("distilbert-base-uncased")
            models["url_model"] = AutoModelForSequenceClassification.from_pretrained(
                "distilbert-base-uncased-finetuned-sst-2-english"
            )
            print("URL model loaded")
        except Exception as e:
            print(f"Failed to load URL model: {e}")

async def load_image_model():
    if models["image_processor"] is None or models["image_model"] is None:
        try:
            models["image_processor"] = ViTImageProcessor.from_pretrained("Wvolf/ViT_Deepfake_Detection")
            models["image_model"] = ViTForImageClassification.from_pretrained("Wvolf/ViT_Deepfake_Detection")
            print("Image model loaded")
        except Exception as e:
            print(f"Failed to load image model: {e}")

# Endpoints - Updated for news analysis
@app.post("/analyze/news")  # New endpoint specifically for news
async def analyze_news(request: NewsAnalysisRequest):
    """Analyze news article for fake news detection using DeBERTa model"""
    await load_news_model()
    if not models["news_classifier"]:
        return {"result": mock_news_analysis("news", request.text)}
    try:
        result = await asyncio.to_thread(models["news_classifier"], request.text)
        return {"result": format_news_result("news", result, request.text)}
    except Exception as e:
        print(f"News analysis error: {e}")
        return {"result": mock_news_analysis("news", request.text)}

@app.post("/analyze/text")  # Keep original endpoint for backwards compatibility
async def analyze_text(request: NewsAnalysisRequest):
    """Analyze text content - now redirects to news analysis"""
    return await analyze_news(request)

@app.post("/analyze/url")
async def analyze_url(request: UrlAnalysisRequest):
    await load_url_model()
    if not models["url_model"]:
        return {"result": mock_analysis("url", request.url)}
    try:
        inputs = models["url_tokenizer"](request.url, return_tensors="pt")
        with torch.no_grad():
            outputs = models["url_model"](**inputs)
        logits = outputs.logits
        probs = torch.nn.functional.softmax(logits, dim=1)
        pred_id = torch.argmax(probs).item()
        label = models["url_model"].config.id2label[pred_id]
        score = probs[0][pred_id].item()
        return {"result": format_result("url", [{"label": label, "score": score}], request.url)}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"URL analysis failed: {str(e)}")

@app.post("/analyze/image")
async def analyze_image(file: UploadFile = File(...)):
    await load_image_model()
    if not models["image_model"]:
        return {"result": mock_analysis("image", file.filename)}
    try:
        image_bytes = await file.read()
        image = Image.open(io.BytesIO(image_bytes))
        inputs = models["image_processor"](images=image, return_tensors="pt")
        with torch.no_grad():
            outputs = models["image_model"](**inputs)
        logits = outputs.logits
        pred_id = logits.argmax(-1).item()
        label = models["image_model"].config.id2label[pred_id]
        score = torch.nn.functional.softmax(logits, dim=1)[0][pred_id].item()
        return {"result": format_result("image", {"label": label, "score": score}, file.filename)}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Image analysis failed: {str(e)}")

@app.post("/analyze/video")
async def analyze_video():
    raise HTTPException(status_code=501, detail="Video analysis not yet implemented")

# Health check endpoint
@app.get("/health")
async def health_check():
    return {
        "status": "healthy",
        "models_loaded": {
            "news_classifier": models["news_classifier"] is not None,
            "url_model": models["url_model"] is not None,
            "image_model": models["image_model"] is not None,
        }
    }
